"""Training module"""

import pickle

import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV

DATASET_PATH = "data/dataset.csv"
CORRELATION_THRESHOLD = 0.8
MODEL_SEARCH_SPACE = [
    {
        "model": SVC,
        "grid": {
            "C": np.arange(2, 10, 1),
            "gamma": np.arange(0.1, 1, 0.1),
        },
    },
    {
        "model": LogisticRegression,
        "grid": {
            'C': np.arange(0, 1, 0.01),
            'max_iter': range(100, 500),
            'warm_start': [True, False],
            'solver': ['lbfgs', 'newton-cg', 'liblinear'],
        },
    },
    {
        "model": DecisionTreeClassifier,
        "grid": {
            "max_depth": [int(x) for x in np.linspace(10, 110, num=11)],
            "max_features": [1, 2, 4],
            "min_samples_leaf": [int(x) for x in np.linspace(start=10, stop=100, num=10)],
            "criterion": ["gini", "entropy"],
        },
    },
    {
        "model": RandomForestClassifier,
        "grid": {
            'bootstrap': [True, False],
            'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],
            'min_samples_leaf': [1, 2, 4],
            'min_samples_split': [1, 2, 4],
            'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
        },
    },
]
MODEL_PATH = "data/model.bin"
FEATURES_PATH = "data/features.bin"


def read_dataset(path):
    """Dataset loading function"""
    df = pd.read_csv(path)
    return df


def prepare_dataset(df):
    """Dataset preparation function"""
    y_train = df["malware"].values
    X_train = df.drop(["hash", "malware"], axis=1)
    return X_train, y_train


def reduce_features(df, threshold):
    """Feature reduction function by correlation"""
    df = df.copy()
    cor_matrix = df.corr().abs()
    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
    df = df.drop(to_drop, axis=1)
    return df, to_drop


def search_best_model(X_train, y_train, search_space):
    """Best model search function"""
    for item in search_space:
        model = item["model"]
        grid = item["grid"]
        print(f"Analysing model: {model}")
        best_params, best_score = analyze_candidate_model(X_train, y_train, model(), grid)
        item["best_params"] = best_params
        item["best_score"] = best_score

    best_model, best_params = get_best_model(search_space)

    return best_model, best_params


def get_best_model(search_space):
    """Analysed models sorting function"""
    sorted_search_space = sorted(search_space, key=lambda x: x["best_score"], reverse=True)
    best_model = sorted_search_space[0]["model"]
    best_params = sorted_search_space[0]["best_params"]
    best_score = sorted_search_space[0]["best_score"]
    print(f"Best model: {best_model}, parameters: {best_params}, score: {best_score}")
    return best_model, best_params


def analyze_candidate_model(X_train, y_train, model, grid):
    """Candidate model analysis function"""
    rand_search = RandomizedSearchCV(
        model,
        param_distributions=grid,
        n_iter=10,
        n_jobs=-1,
        cv=3,
        random_state=42,
        scoring='roc_auc',
        verbose=2,
    )
    rand_search.fit(X_train, y_train)
    return rand_search.best_params_, rand_search.best_score_


def train_best_model(model, params, X_train, y_train):
    """Best model training function"""
    print("Training model")
    model = model(**params)
    model.fit(X_train, y_train)
    return model


def evaluate_model(model, X_train, y_train):
    """Model evaluation function"""
    y_pred = model.predict(X_train)
    print("Accuracy: ", accuracy_score(y_train, y_pred))
    print("Precision:", precision_score(y_train, y_pred))
    print("Recall:", recall_score(y_train, y_pred))
    print("F1 Score:", f1_score(y_train, y_pred))


def export_model(model, path):
    """Export model function"""
    with open(path, "wb") as f:
        pickle.dump(model, f)


def export_features(X_train, path):
    """Export features function"""
    with open(path, "wb") as f:
        pickle.dump(X_train.columns.to_list(), f)


def main():
    """Main function"""
    print("Loading dataset")
    df = read_dataset(path=DATASET_PATH)

    print("Preparing dataset")
    X_train, y_train = prepare_dataset(df)

    print("Reducing features by correlation")
    X_train, _ = reduce_features(X_train, threshold=CORRELATION_THRESHOLD)

    print("Searching for he best model")
    best_model, best_params = search_best_model(X_train, y_train, search_space=MODEL_SEARCH_SPACE)

    print("Training the best model")
    trained_model = train_best_model(best_model, best_params, X_train, y_train)

    print("Showing model performance indicators")
    evaluate_model(trained_model, X_train, y_train)

    print("Exporting model and features")
    export_model(trained_model, path=MODEL_PATH)
    export_features(X_train, path=FEATURES_PATH)


if __name__ == "__main__":
    main()
